1. Title
2. Background and Motivation
3. Research Objectives
4. System Model 
5. Task-parallel vs. Data-parallel
6. Data-parallel
7. Selective Accleration: Acceleration gainκ³Ό Blocking Loss Net Gain
System: M CPU cores {C 1 β€‹ ,C 2 β€‹ ,β€¦,C M} + single shared GPU (G)
DNN Task: N layers (l i) with CPU/GPU execution time (ci, gi)
Layer-to-Resource Mapping: π™ π‘– = { 0 (CPU) , 1 (GPU) }
Total execution time: einferβ€‹(Ο•)=i=1β‘Nβ€‹[ciβ€‹β‹…(1β’Ο•iβ€‹)+giβ€‹β‹…Ο•iβ€‹]
8. Selective Acceleration: μ–Όλ§λ‚ λ§μ΄ κ°€μ†? ν•λƒμ— λ”°λΌ Acceleration gainκ³Ό Blocking Loss μ°¨μ΄
9. Selective Acceleration: λ„κµ¬λ¥Ό κ°€μ†? ν•λƒμ— λ”°λΌ Acceleration gainκ³Ό Blocking Loss μ°¨μ΄
10. Blocking Loss Analysis: WCRT and BCRT
11. λ‹¨κ³„λ³„ μ „λµ(Stepwise μ „λµ): DNN λ μ΄μ–΄ μ(N)κ°€ λ§μ•„μ§μλ΅ κ°€λ¥ν• CPU/GPU λ°°μΉμ μ΅°ν•© μλ” μ§€μμ μΌλ΅ μ¦κ°€ν•μ—¬(2N2^N2N) μ™„μ „ νƒμƒ‰(Exhaustive search)μ€ ν„μ‹¤μ μΌλ΅ λ¶κ°€λ¥ν•¨.
λ”°λΌμ„ ν„μ‹¤μ μ΄κ³  ν¨μ¨μ μΈ νƒμƒ‰μ„ μ„ν•΄ λ‹¨κ³„λ³„ μ „λµ(Stepwise Selective Acceleration Heuristics) μ„ ν†µν•΄ μµμ μ λ°°μΉλ¥Ό λΉ λ¥΄κ² μ°Ύμ•„μ•Ό ν•¨.
12. Full-freedom μ „λµ (2^N combination):
λ¨λ“  κ°€λ¥ν• λ°°μΉλ¥Ό νƒμƒ‰ν•λ” λ°©μ‹.
κ°€μ¥ μ μ—°ν•μ§€λ§ ν„μ‹¤μ μΌλ΅ κ³„μ‚° λΉ„μ©μ΄ λ„λ¬΄ λ†’μΌλ©°, Full-freedom μ „λµμ€ λ μ΄μ–΄ κ°„ GPU/CPU μ „ν™μ΄ λΉλ²ν λ°μƒν•μ—¬ GPU Idle timeκ³Ό blocking timeμ΄ λΉλ²ν•κ² μƒκΉλ‹λ‹¤. μ΄λ΅ μΈν•΄ μ‹¤μ  Blocking Lossκ°€ μ¦κ°€ν•κ² λ©λ‹λ‹¤. GPU κ°€μ†μ΄ κ³Όλ„ν•κ² ννΈν™”(fragmented) λμ–΄ Blocking Loss μ¦κ°€λ΅ ν¨μ¨μ΄ λ‚®μ.
λ™μΌν• GPU κ°€μ†λ‰(Acceleration Gain)μ„ κ°€μ§€λ” μƒν™©μ—μ„λ„ μ–΄λ–¤ λ μ΄μ–΄λ¥Ό GPUμ— λ§¤ν•‘ν•λλƒμ— λ”°λΌ Blocking Lossκ°€ ν¬κ² λ‹¬λΌμ§€λ―€λ΅, μ΄λ¥Ό λ…ν™•ν μ •λ‰ν™”ν•μ—¬ λ¶„μ„ν•λ” κ²ƒμ΄ ν•„μμ μ„. (GPU κ°€μ† ννΈν™”λ” μΌλ°μ μΌλ΅ λ¶λ¦¬ν•¨) --> νƒ€μ΄λ° λ‹¤μ΄μ–΄κ·Έλ¨μΌλ΅ λΉ„κµν•΄μ„ Blocking Lossκ°€ μ»¤μ§€λ” κ²ƒμ„ λ³΄μ—¬μ¤ (Multi-GPU Segment, Single-GPU Segment μ°¨μ΄ λ³΄μ—¬μ£Όλ©΄μ„ Single μ „λµμΈ Targeted AccelerationμΌλ΅ μμ—°μ¤λ½κ² λ„μ–΄κ°)
13. Targeted Acceleration [Start, k] (N(N+1)/2 combination):
μ„μμ λ μ΄μ–΄μ—μ„ μ‹μ‘ν•μ—¬ μ—°μ†λ λ μ΄μ–΄λ¥Ό GPUλ΅ κ°€μ†ν™”ν•λ” λ°©μ‹. μ μ—°ν•λ©΄μ„λ„ νƒμƒ‰ κ³µκ°„μ„ ν„μ €ν μ¤„μ—¬ ν¨μ¨μ μ„.
Layer λ³„ λ‹¤μ–‘ν• Computation νΉμ„±μ„ κ°€μ§„ DNNμ—μ„ μΆ‹μ€ μ„±λ¥μ„ κΈ°λ€ν•  μ μμ.
Targeted Accelerationμ€ μƒλ‹Ήν ν¨μ¨μ μ΄μ§€λ§, λ€λ¶€λ¶„μ λ¨λΈμ—μ„ μ‹¤μ  μ—°μ‚° λ¶€ν•κ°€ μ΄λ°λ¶€ λ μ΄μ–΄μ— μ§‘μ¤‘λμ–΄ μμΌλ―€λ΅ μ΄λ¥Ό κ³ λ ¤ν• Front-first μ „λµμ΄ λ”μ± ν„μ‹¤μ μ…λ‹λ‹¤. μ—°μ‚°λ‰μ΄ νΉμ • μμ—­(νΉν μ΄λ°λ¶€)μ— λ°λ¦° λ¨λΈμ κ²½μ° λ°λ“μ‹ μµμ μ΄ μ•„λ‹ μ μμ.
14. Front-first μ „λµ [0, k] (N combination):
μ—°μ‚°λ‰μ΄ λ§μ€ μ΄λ°λ¶€ λ μ΄μ–΄λ¥Ό μ°μ„  GPU κ°€μ†ν™” ν•λ” λ°©μ‹.
μΌλ°μ μΌλ΅ DNN λ¨λΈλ“¤μ€ μ΄λ° λ μ΄μ–΄κ°€ κ°€μ¥ μ—°μ‚°μ΄ μ§‘μ¤‘λμ–΄ μμΌλ―€λ΅ ν„μ‹¤μ μ΄κ³  ν¨μ¨μ μΈ μ „λµμ„.
μ‹¤ν—μ μΌλ΅ DenseNetκ³Ό κ°™μ€ λ¨λΈμ—μ„λ” κ°€μ¥ μ„±λ¥μ΄ λ›°μ–΄λ‚ μ „λµμΌλ΅ μ…μ¦λ¨.
15. Step-wise μ „λµ Summary:
μ‚¬μ§„ μ°Έκ³ 
16. Experimental Setup
17. Evaluation: Full-Freedom Acceleration
18. Evaluation: Targeted Acceleration
19. Evaluation: Front-First Acceleration
20. Conclusions and Future Work